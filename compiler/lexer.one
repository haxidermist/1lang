// lexer.one - Lexer for the 1 programming language
// This is the self-hosting compiler component

// Token types as constants
function TOKEN_IDENTIFIER:
  outputs:
    t: Integer
  implementation:
    return 1

function TOKEN_INTEGER:
  outputs:
    t: Integer
  implementation:
    return 2

function TOKEN_STRING:
  outputs:
    t: Integer
  implementation:
    return 3

function TOKEN_KEYWORD:
  outputs:
    t: Integer
  implementation:
    return 4

function TOKEN_OPERATOR:
  outputs:
    t: Integer
  implementation:
    return 5

function TOKEN_DELIMITER:
  outputs:
    t: Integer
  implementation:
    return 6

function TOKEN_EOF:
  outputs:
    t: Integer
  implementation:
    return 7

// Token structure: [type, lexeme, line, column]
function create_token:
  inputs:
    token_type: Integer
    lexeme: String
    line: Integer
    column: Integer
  outputs:
    token: Integer
  implementation:
    // For now, just return type
    // In future, we'll return a proper structure
    return token_type

// Check if character is whitespace
function is_whitespace:
  inputs:
    c: String
  outputs:
    result: Integer
  implementation:
    if str_eq(c, " "):
      return 1
    else:
      if str_eq(c, "\t"):
        return 1
      else:
        if str_eq(c, "\n"):
          return 1
        else:
          return 0

// Check if character is alpha
function is_alpha_char:
  inputs:
    c: String
  outputs:
    result: Integer
  implementation:
    alpha = is_alpha(c)
    underscore = str_eq(c, "_")
    if alpha:
      return 1
    else:
      if underscore:
        return 1
      else:
        return 0

// Check if character is alphanumeric
function is_alnum_char:
  inputs:
    c: String
  outputs:
    result: Integer
  implementation:
    alnum = is_alnum(c)
    underscore = str_eq(c, "_")
    if alnum:
      return 1
    else:
      if underscore:
        return 1
      else:
        return 0

// Simple tokenizer - returns list of tokens
function tokenize:
  inputs:
    source: String
  outputs:
    tokens: Integer
  implementation:
    // Initialize
    pos = 0
    line = 1
    column = 1
    source_len = len(source)

    // Main tokenization loop
    while pos < source_len:
      c = char_at(source, pos)

      // Skip whitespace
      ws = is_whitespace(c)
      if ws == 1:
        if str_eq(c, "\n"):
          line = line + 1
          column = 1
        else:
          column = column + 1
        pos = pos + 1
      else:
        // Check for identifiers and keywords
        alpha = is_alpha_char(c)
        if alpha == 1:
          start = pos
          start_col = column

          // Consume identifier
          while pos < source_len:
            c2 = char_at(source, pos)
            alnum = is_alnum_char(c2)
            if alnum == 1:
              pos = pos + 1
              column = column + 1
            else:
              break

          // Extract lexeme
          lexeme = substr(source, start, pos)

          // Create token (simplified - just count for now)
          println(lexeme)

        else:
          // Check for numbers
          digit = is_digit(c)
          if digit == 1:
            start = pos
            start_col = column

            // Consume digits
            while pos < source_len:
              c2 = char_at(source, pos)
              d = is_digit(c2)
              if d == 1:
                pos = pos + 1
                column = column + 1
              else:
                break

            // Extract number
            lexeme = substr(source, start, pos)
            println(lexeme)

          else:
            // Single character tokens
            println(c)
            pos = pos + 1
            column = column + 1

    return 0

// Test function
function main:
  outputs:
    exit_code: Integer
  implementation:
    println("Lexer Test")

    // Test tokenization
    code = "function add"
    result = tokenize(code)

    return 0
